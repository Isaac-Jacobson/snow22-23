{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SnowflakeClassifier",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaac-Jacobson/snow/blob/main/SnowflakeClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Snowflake Detector"
      ],
      "metadata": {
        "id": "R13NHesM1bRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bonus cell just for executing linux commands\n",
        "\n",
        "!pip uninstall opencv-python-headless==4.5.5.62 \n",
        "!pip install opencv-python-headless==4.1.2.30\n",
        "\n"
      ],
      "metadata": {
        "id": "WiJKNB-Y1Yrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "TbZSNoQ31n84"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "source": [
        "#Install dependecies\n",
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Needed imports although matplotlib, numpy,and pandas aren't currently used but will probably be needed\n",
        "import tensorflow as tf\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "899Xah1s-PVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get the data and base model"
      ],
      "metadata": {
        "id": "l9SrNd3b2Suu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "source": [
        "#Picking what base model to use, efficientdet is just a starting place\n",
        "#spec = model_spec.get('efficientdet_lite0')\n",
        "#spec = model_spec.get('efficientdet_lite2')\n",
        "spec = model_spec.get('efficientdet_lite4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dat data\n",
        "#!curl -L \"https://app.roboflow.com/ds/fosD79eC34?key=xH3OhXG8fK\" > data.zip\n",
        "#!unzip data.zip; rm data.zip\n",
        "!curl -L \"https://app.roboflow.com/ds/1XRSjPxvAk?key=B8s0tsnsPH\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "metadata": {
        "id": "UrHyUD4voZq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm working on automating the jpeg and csv manipulation but right now I still hand format the csv\n",
        "\n",
        "#!mkdir data\n",
        "!mv ./test/*.jpg .\n",
        "!mv ./train/*.jpg .\n",
        "!mv ./valid/*.jpg .\n",
        "#!mkdir annotations\n",
        "#!mv ./test/*.csv ./annotations/test.csv\n",
        "#!mv ./train/*.csv ./annotations/train.csv\n",
        "#!mv ./valid/*.csv ./annotations/valid.csv\n",
        "#!rm ./merged.csv\n",
        "#!head -n 1 ./annotation/train.csv > merged.csv && tail -n+2 -q ./annotation/*.csv >> merged.csv"
      ],
      "metadata": {
        "id": "U1VSEsVvqwJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and test"
      ],
      "metadata": {
        "id": "st34ytea2X8R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD5BvzWe6YKa"
      },
      "source": [
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('./annotations.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data.label_map)\n",
        "\n",
        "#Output should be: {1: 'class', 2: 'Snowflake'}"
      ],
      "metadata": {
        "id": "7q0505gI39x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "source": [
        "# train_whole_model, controls layers being trained, setting to false uses transfer learning to train and\n",
        "# only trains layers that don't match model_spec.config.var_freeze_expr.\n",
        "model = object_detector.create(train_data, model_spec=spec, epochs = 5, batch_size=1, train_whole_model=True, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "#There should be 15,108,198 parameters if using lite4"
      ],
      "metadata": {
        "id": "jNayYrKp3Suj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "vDIhcO_w2gQR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "source": [
        "#Needs a bigger test set\n",
        "\n",
        "#Prints mAP for whole model and specifically for each piece (class)\n",
        "model.evaluate(test_data, batch_size=1)\n",
        "#print (model.predict(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making and Testing the tflite version"
      ],
      "metadata": {
        "id": "_SwUpxN02lZZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "source": [
        "# Defaults to post training full integer quantization when exported to tflite file\n",
        "model.export(export_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYDWcljr6jq"
      },
      "source": [
        "#Prints mAP for whole model and specifically for each piece (class)\n",
        "model.evaluate_tflite('model.tflite', test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "model_path = 'model.tflite'\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['???'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "  classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "  return resized_img, original_image\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "\n",
        "  signature_fn = interpreter.get_signature_runner()\n",
        "\n",
        "  # Feed the input image to the model\n",
        "  output = signature_fn(images=image)\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  count = int(np.squeeze(output['output_0']))\n",
        "  scores = np.squeeze(output['output_1'])\n",
        "  classes = np.squeeze(output['output_2'])\n",
        "  boxes = np.squeeze(output['output_3'])\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path,\n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute\n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ],
      "metadata": {
        "id": "5DzuY7bIqvwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ../image.png"
      ],
      "metadata": {
        "id": "ZjXnvDhIH6oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_IMAGE_URL = \"https://lh3.googleusercontent.com/80TAra6bhWnjfpprd9HHquVO5aCmvohGowH--cHZxiD1npvL4BlDj2n3pJvt84z9grG8I-nwED-yCuE6R8INPi-HfKv7Ua5NWCF2Xo7K1BkAfTk1Jpu1aAXyqS0-aXPF5TVwhjRG8KhYksP_VPaGecpAvZpBGKNns6SzfMRboX2SOMGWZBcFMrR0OXNe4wIYtaJvnz4biubq6b1omPZK0PCfoAVhLC05ATX6j4W0V_MthZ3FjJrdP5VJNKe94_ki4cUkcc4B4g1Oicd-yhWl0IaF3xbLKi13YPmtiitxYK0nCpDnH5BF3frzEX7r3dwG7zX3sdBPrPlSGB_Ki2gDYNOI6E8IPkQzhbakHD6QwNztsAfmQyP3LkpOE8dHf7FOqdvuFjrJy2vWCuIsr0lgUdgxcUqfUE0qG4iIUoJG1Pkyy9l54lfSikLbuSk6uKc1AzkUo0otZrwH9uzv_6isMCy1frynhHoP77iVsDXGUWXEHavntWkxlyzE81HxSe5xoai4suv6CwUZAL03vaJDaUuM45xw3kXk0pk_yewFLTtMuyPpJJd1aDpBT8eUHSWq72yruHt-jfPb5f5jGmihf3GetoYbq8mGdXpYvyN4buCZd8ztUDSw_y_fFSvmn7K8b0txIEaHbdZ-EhStABaYqndcBbyv9P6LP23j64WgV16J89lsyuTYLB0q_g4NQ7ph4JZNgzHhR6Y8qCxMmeNVUDCY=w1068-h893-no?authuser=0\"\n",
        "DETECTION_THRESHOLD = 0.2\n",
        "\n",
        "TEMP_FILE = '/image.png'\n",
        "\n",
        "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "im = Image.open(TEMP_FILE)\n",
        "im.thumbnail((2048, 1714), Image.ANTIALIAS)\n",
        "im.save(TEMP_FILE, 'PNG')\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Run inference and draw detection result on the local copy of the original file\n",
        "detection_result_image = run_odt_and_draw_results(\n",
        "    TEMP_FILE,\n",
        "    interpreter,\n",
        "    threshold=DETECTION_THRESHOLD\n",
        ")\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(detection_result_image)\n"
      ],
      "metadata": {
        "id": "9z7OIt_kq0gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unused at the moment ****************************\n",
        "\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "id": "sLyzjZ48p0d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount google drive for exports"
      ],
      "metadata": {
        "id": "YBhWyEhr2q0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#link drive for easy saving, although just downloading the model is easier\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5weWv68yCFv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function for drawing a bounded box on an image\n",
        "def draw_rect(image, box):\n",
        "    y_min = int(max(1, (box[0] * image.height)))\n",
        "    x_min = int(max(1, (box[1] * image.width)))\n",
        "    y_max = int(min(image.height, (box[2] * image.height)))\n",
        "    x_max = int(min(image.width, (box[3] * image.width)))\n",
        "    \n",
        "    # draw a rectangle on the image\n",
        "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 255, 255), 2)"
      ],
      "metadata": {
        "id": "ri3QSSAf-XMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
